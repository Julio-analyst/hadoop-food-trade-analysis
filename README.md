# ğŸ“ Implementasi Ekosistem Hadoop untuk Analisis Perdagangan Internasional Bahan Pangan (2024)

> ğŸš€ Big data pipeline for batch processing of food import datasets using Hadoop ecosystem, Spark, and Superset.

---

## ğŸ“Œ Overview

This project builds a **Hadoop-based data platform** to analyze Indonesia's international food import data. It leverages **batch processing**, **Medallion architecture**, and a modern open-source stack for scalable data ingestion, transformation, and visualization.

- ğŸŒ Dataset: UN Comtrade & FAOSTAT (food import data)
- ğŸ¢ Stack: Hadoop, Spark, Hive, Superset, Docker
- ğŸ¤– Goal: End-to-end pipeline from raw data to dashboard

---

## ğŸ’¼ Team

Developed by **Kelompok 13 â€“ Institut Teknologi Sumatera (Prodi Sains Data)**
- Rangga Adi Putra
- Cyntia Kristina Sidauruk
- Azizah Kusumah Putri
- Farrel Julio Akbar

---

## ğŸ› ï¸ System Architecture

### ğŸ”„ Medallion Architecture

| Layer  | Description                                     | Format           | Tools             |
|--------|--------------------------------------------------|------------------|--------------------|
| Bronze | Raw external data                               | CSV / JSON       | HDFS               |
| Silver | Cleaned, structured data                        | Parquet / ORC    | Spark, Hive        |
| Gold   | Aggregated, analytics-ready data                | Parquet / ORC    | Hive, Superset     |

### ğŸ—ï¸ Stack Components

- **HDFS** â€“ Distributed file storage
- **Apache Spark** â€“ ETL, transformation, MLlib
- **Apache Hive** â€“ SQL engine and table registry
- **Apache Superset** â€“ Dashboard visualization
- **Docker Compose** â€“ Local cluster orchestration
- **Crontab / Airflow** â€“ Batch job scheduling

---

## ğŸ“ˆ Datasets

- **UN Comtrade** â€“ Global food trade data
- **FAOSTAT** â€“ Food agriculture stats (volume/value)

Key fields:
- Year (refYear)
- Country (reporter, partner)
- Commodity code (HS Code)
- Import value (cifvalue), quantity (qty), weight (netWgt)
- Metadata & source flags

---

## ğŸ”„ Data Pipeline

```
1. Fetch data via API / curl
2. Store in HDFS (Bronze)
3. Transform & clean via Spark (Silver)
4. Aggregate via Hive (Gold)
5. Register Hive tables
6. Visualize via Superset dashboard
```

---

## ğŸ“ Folder Structure

```
/opt/bigdata/
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ bronze/       # Raw CSVs
â”‚   â”œâ”€â”€ silver/       # Cleaned Parquet
â”‚   â””â”€â”€ gold/         # Aggregated OLAP results
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ ingest.sh     # Data ingestion script
â”‚   â””â”€â”€ etl_spark.py  # Spark ETL transformation
â””â”€â”€ logs/             # ETL logs
```

---

## ğŸ”§ Technologies Used

| Component       | Function                            |
|----------------|-------------------------------------|
| Docker Compose | Virtual Hadoop cluster orchestration |
| Hive Metastore | Table metadata registry              |
| Spark Submit   | Run ETL batch jobs                   |
| Superset       | Build interactive dashboards         |
| Crontab        | Job automation                       |

---

## ğŸ” Features

- âœ… Automated batch ingestion
- âœ… Clean & efficient Parquet transformation
- âœ… SQL-ready Hive tables
- âœ… Interactive dashboard via Superset
- âœ… ML-ready data for Spark MLlib

---

## ğŸ“… Testing Strategy

| Test Type           | Purpose                                |
|---------------------|----------------------------------------|
| Unit Test           | Validate ETL script functions           |
| Integration Test    | Ingestion â” Spark â” Hive linkage         |
| Data Quality Test   | Handle nulls, duplicates, schema check |
| Performance Test    | Monitor batch execution times          |
| End-to-End Test     | Verify from fetch to Superset view     |

---

## ğŸ¤– Advanced Analytics (Optional MLlib)

- Use **Spark MLlib regression** to predict future import volume
- Feature engineering to analyze influential factors
- Save prediction output to Hive Gold layer

---

## ğŸš€ Deployment Guide (Local)

1. **Install Docker Desktop**
2. Clone repository:
```bash
git clone https://github.com/sains-data/Analisis-Impor-Bahan-Pangan-dari-Global
cd Analisis-Impor-Bahan-Pangan-dari-Global
```
3. Launch Hadoop stack:
```bash
docker-compose up -d
```
4. Run pipeline:
```bash
bash scripts/ingest.sh
spark-submit scripts/etl_spark.py
```
5. Open Superset: `http://localhost:8088`

---

## ğŸ“„ License

MIT License â€” Academic purpose (Institut Teknologi Sumatera)  
Open for learning, with credit to original authors

---

## ğŸ“– References

1. [UN Comtrade](https://comtrade.un.org/)
2. [FAOSTAT](https://www.fao.org/faostat)
3. [Apache Hadoop](https://hadoop.apache.org/)
4. [Apache Spark](https://spark.apache.org/)
5. [Apache Hive](https://hive.apache.org/)
6. [Apache Superset](https://superset.apache.org/)
